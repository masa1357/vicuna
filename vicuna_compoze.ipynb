{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vicuna 圧縮テスト\n",
    "Vicuna_13bを無理やり圧縮して使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'A1-1.pdf', 'text': '# ニューラル機械翻訳における Iterative Back-Translation を利用した コンパラブルコーパスの活用\\n\\n山本 優紀 秋葉 友良 塚田 元\\n豊橋技術科学大学\\n\\\\{yamamoto.yuki.pr, akiba.tomoyoshi.tk, tsukada.hajime.hl\\\\}@tut.jp\\n\\n## 概要\\n\\nニューラル機械翻訳 (NMT) の学習に用いる対訳コーパスの構築法として, 文書単位で対応付けられた 2 つの言語のコーパス (コンパラブルコーパス) から、対応付けられる文ペアを自動的に抽出する手法が広く採用されている. しかし, 文単位で意味が対応するものは少なく,多くの文は抽出されず捨てられてしまう. 本研究では、対訳コーパスとして抽出されなかった文を含めて,コンパラブルコー パス全体を NMT の学習に活用する手法を提案する. 評価実験により, コンパラブルコーパスでデータ拡張を行うことや, コンパラブル性の利用, Iterative Back-Translation の活用によって翻訳モデルの性能が向上することを確認した.\\n\\n## 1 はじめに\\n\\n機械翻訳の分野では, 深層学習の発達により, ニューラルネットワークを用いるニューラル機械翻訳 (Neural Machine Translation:NMT) が, 従来手法の統計的機械翻訳よりも高い性能を示しており, 様々な研究が行われている. NMT では, ニューラルネットワークで構築した翻訳モデルを, 翻訳元の言語 (原言語) の文と,その訳の言語 (目的言語) の文のぺアにした対訳コーパスを用いて学習を行う. NMT は, 対訳コーパスから翻訳に関わる様々な知識を学習するため, 対訳コーパスの質や量が NMT モデルの翻訳性能に大きく影響する.しかし, 大規模な対訳コーパスを人手で作成することは困難という問題点がある.\\n\\nこの問題の解決策として, 既存の日本語と英語の翻訳テキストから対訳コーパスを構築する手法が提案されている.[1]これは, 新聞などの文書単位で対応付けつけられた 2 つの言語コーパス (コンパラブルコーパス) から, 対応付けられる文ぺアを自動的\\nに抽出することで対訳コーパスを構築する方法である. しかし,コンパラブルコーパスの中で文単位で意味が対応するものは少なく,多くの文は抽出されずに捨てられてしまう. 実際, 本論文で使用した PatentMT の調査では 1 つの文書から平均約 $27.1 \\\\%$ の文しか抽出されていなかった.\\n\\n本研究では, 対訳コーパスとして抽出されなかった文を含めて,コンパラブルコーパス全体を NMT の学習に活用する手法を提案する. データ拡張手法として, 逆翻訳 (Back-Translation:BT)[2] や, その拡張手法である Iterative Back-Translation (IBT)[3][4][5] を利用することで，より効果的なデータ拡張手法を探す. さらに, 上記の手法をコンパラブルコーパスのコンパラブル性を活用して行い, その効果を調べる.\\n\\n## 2 提案手法\\n\\n## 2.1 コンパラブルコーパスの再現\\n\\n本研究では, 対訳コーパスの抽出元であるコンパラブルコーパスを翻訳モデル学習に活用することを目的とする. しかし, 実験で用いる NTCIR-10 PatentMT[6] のコンパラブルコーパスを直接入手することができなかったため, 以下の方法で対訳コー パスからコンパラブルコーパスを再現した.\\n1. $C=\\\\{\\\\}$ と初期化する.\\n\\n2. 対訳コーパス $P$ の各文ペア $(x, y) \\\\in P$ について以下を繰り返す。\\n\\n$2.1 x$ と $y$ の抽出元の文書である $D_{x}$ と $D_{y}$ を特定する。\\n\\n2.2 特定した $D_{x}$ と $D_{y}$ を文書ペア $\\\\left(D_{x}, D_{y}\\\\right)$ とし, $C$ に $C \\\\leftarrow C \\\\bigcup\\\\left.\\\\{\\\\left(D_{x}, D_{y}\\\\right)\\\\right.\\\\}$ と追加する.\\n\\n最終的にコンパラブルコーパス $C=$ $\\\\bigcup_{(x, y) \\\\in P}\\\\left.\\\\{\\\\left(D_{x}, D_{y}\\\\right)\\\\right.\\\\}$ が得られる.\\n\\n## 2.2 データ拡張手法\\n\\n節 2.1 で構築したコンパラブルコーパスを利用して, データ拡張を行う. 本研究では, 4 つの手法でデータ拡張実験を行い, 比較を行うことで, より効果的なコンパラブルコーパスの活用方法を模索する.\\n\\n## 2.2.1 Back-Translation\\n\\n逆翻訳手法 (Back-Translation:BT) は, Sennrich ら [2] の提案した手法である. BT の流れを図 1 に示す. 図 1 では, 言語 $X$ から言語 $Y$ の翻訳モデルの構築を考えている. はじめに, 対訳コーパスを利用して $Y \\\\rightarrow X$ 方向の翻訳モデル Model $_{Y \\\\rightarrow X} 0$ を作成する.次に,このモデルを用いて, 単言語コーパス $C_{Y}$ mono からサンプリングして得たサブセット $\\\\hat{C}_{Y}$ mono を逆翻訳し, 翻訳結果 $\\\\hat{C}_{X}^{\\\\prime}$ mono を得る. 翻訳結果と元の単言語コーパスを組み合わせて疑似対訳コーパス ( $\\\\hat{C}_{X}^{\\\\prime}$ mono, $\\\\hat{C}_{Y}$ mono $)$ を構築する. 構築した疑似対訳コーパスと対訳コーパスを混合し, 言語 $X$ から言語 $Y$ の翻訳モデル Model $_{X \\\\rightarrow Y} 1$ を学習する. 以上が BT の流れである. 本研究では, 構築したコンパラブルコーパス $C=\\\\bigcup_{(x, y) \\\\in P}\\\\left.\\\\{\\\\left(D_{x}, D_{y}\\\\right)\\\\right.\\\\}$ の Y 言語側 $C_{Y}=\\\\bigcup_{(x, y) \\\\in P}\\\\left.\\\\{D_{y}\\\\right.\\\\}$ を単言語コーパスとすることで BTを利用する。\\n\\n図 1 Back Translation\\n\\n## 2.2.2 Iterative Back-Translation\\n\\nIterative Back-Translation(IBT) は, 原言語の単言語コーパスと目的言語の単言語コーパスを用いて, BT を双方向かつ反復的に繰り返す手法である. IBT の流れを図 2 に示す. 図では, 言語 $X$ と言語 $Y$ における IBT の流れを示している. IBT は以下のようにしてモデルを学習する。\\n\\n1. 対訳コーパスを用いて, $X \\\\rightarrow Y, Y \\\\rightarrow X$ の各方向の翻訳モデル Model $_{X \\\\rightarrow Y} 0$, Model $_{Y \\\\rightarrow X} 0$ を学習し, $i \\\\leftarrow 0$ に初期化する.\\n\\n2. 以下の手順で Model $_{X \\\\rightarrow Y} i$ を更新する.\\n\\n2.1 Model $_{Y \\\\rightarrow X} i$ で単言語コーパス $C_{Y}$ mono からサンプリングして得たサブセット $\\\\hat{C}_{Y}$ mono を翻訳し, 疑似対訳コーパス ( $\\\\hat{C}_{X}^{\\\\prime}$ mono, $\\\\hat{C}_{Y}$ mono) を得る.\\n\\n2.2疑似対訳コーパス ( $\\\\hat{C}_{X}^{\\\\prime}$ mono, $\\\\hat{C}_{Y}$ mono) と対訳コーパス $\\\\left(C_{X}, C_{Y}\\\\right)$ を結合し, $\\\\operatorname{Model}_{X \\\\rightarrow Y} i$ を fine-tuning し, $\\\\operatorname{Model}_{X \\\\rightarrow Y}(i+1)$ を学習する。\\n\\n3. ステップ 2 と同様に Model $_{Y \\\\rightarrow X} i$ を更新する.\\n4. $i \\\\leftarrow i+1$ としてステップ 2 に戻る.\\n\\n本研究では, BT と同じように, 構築したコンパラブルコーパスを, 単言語コーパスとすることでIBT を利用する。\\n\\n図 2 Iterative Back-Translation\\n表 1 実験に使用したコーパスサイズ\\n\\n## 2.2.3コンパラブル性を利用した IBT\\n\\nコンパラブル性を利用した IBT では, 構築したコンパラブルコーパスが文書単位で対応付けられていることを利用して, IBT に利用する両言語の単言語コーパスをコンパラブルになるように選択する方法である. 具体的には, IBT のステップ 2.1 および 3.1 で単言語コーパスから $\\\\hat{C}_{X}$ mono および $\\\\hat{C}_{Y}$ mono をサンプリングする際, $\\\\hat{C}_{X}$ mono と $\\\\hat{C}_{Y}$ mono が互いにコンパラブルになるように選ぶ. すなわち, 指定されたサンプリングサイズを満たすように最小限のコンパラブルコーパスのサブセット $C_{s u b}=\\\\left.\\\\{\\\\left(D_{X}, D_{Y}\\\\right)\\\\right.\\\\} \\\\subset C$ をサンプリングして, $\\\\hat{C}_{X}$ mono $\\\\subseteq \\\\cup_{\\\\left(D_{X}, D_{Y}\\\\right) \\\\in C_{\\\\text {sub }}}\\\\left.\\\\{D_{X}\\\\right.\\\\}$ および $\\\\hat{C}_{Y}$ mono $\\\\subseteq \\\\cup_{\\\\left(D_{X}, D_{Y}\\\\right) \\\\in C_{\\\\text {sub }}}\\\\left.\\\\{D_{Y}\\\\right.\\\\}$ のように単言語コーパスを選択する。\\n\\n## 3 評価実験\\n\\n## 3.1 データセット\\n\\n本研究では, 使用する大規模なコーパスとして特許機械翻訳テストコレクションである NTCIR 10 PatentMT[6] を使用した. PatentMT は特許文書から文を抽出することで構築されている対訳コーパスである. PatentMT の対訳コーパスから, 2.1 節の方法でコンパラブルコーパスを構築した. このとき,数式を含む文や長い文を除いた. 使用した対訳コーパスと構築したコンパラブルコーパスのサイズを表 1 に示す.\\n\\nまた, PatentMT の対訳コーパスと構築したコンパラブルコーパスの関係を調査した. コンパラブルコーパスの全文書は 66,414 文書である. このうちの 20,485 文書は, 文書内の $10 \\\\%$ 以下の文しか対訳コー パスとして抽出されていないことがわかった. また,構築したコンパラブルコーパスを利用することで，約 67\\\\%の文を新しく学習に使用することができることがわかった.表 2 コンパラブルコーパスの効果確認実験の結果\\n\\n## 3.2 データセットの前処理\\n\\n前処理として英語文, 日本語文ともに NFKC 正規化を行った. また, 英語文は Moses[7] に付属するトークナイザーと truecaser でトークナイズ大文字小文字の表記を統一した. 学習前の事前処理として, SentencePiece[8] で語彙サイズを 16,000 でサブワー ド化を行った.\\n\\n## 3.3 ニューラル機械翻訳のパラメータ\\n\\nNMT システムには Fairseq[9] の Transformer を使用した. エンコーダー及びデコーダは Transformer を 6 層とした. 学習率は 5e-4 とし, Warmup は 4000 ステップ, dropout は 0.1 としている. 損失関数は, ラべル平滑化クロスエントロピーを使用した. 最適化関数は Adam を利用し, パラメータである $\\\\beta_{1}$ を $0.9, \\\\beta_{2}$ を 0.98 に設定した。\\n\\n## 3.4 コンパラブルコーパスの効果\\n\\n今回構築したコンパラブルコーパスの効果を確認するための実験を行った. PatentMT の対訳コーパスのみで学習した翻訳モデルと,コンパラブルコーパスを利用してデータ拡張を行った翻訳モデルを比較する。\\n\\nベースラインは, PatentMT の対訳コーパスのみで学習したものを利用した. コンパラブルコーパスを利用した翻訳モデルは, ベースラインに加え, 全てのコンパラブルコーパスを利用したものと,対訳コー パスと同サイズである $3,186,254$ 文をコンパラブルコーパスから抽出したものの 2 つで実験を行った. ベースラインを利用してそれぞれ BTを行い, デー 夕拡張して学習を行った. ベースラインは 20epoch, コンパラブルコーパスを利用した翻訳モデルはどちらも 10epoch の学習を行った. 評価尺度は BLEU[10] を用いる。また, NTCIR-10 のベスト翻訳モデルとも比較を行った。\\n\\nコンパラブルコーパスの効果確認の実験結果を表\\n表 3 翻訳モデルの BLEU\\n\\n2 に示す. なお, 表 2 のサイズは, 左が対訳コーパスの使用文数, 右が単言語コーパスの使用文数となっている.\\n\\nコンパラブルコーパスを利用した 2 つの結果がベースラインを上回ったことから，これまで利用されていなかったコンパラブルコーパスを活用することの有効性を示している. また, NTCIR-10 のベスト翻訳モデルと BLEU を比較すると, BLEU を大きく上回っており, 本実験で作成された翻訳モデルは十分な性能があるといえる.\\n\\n## 3.5 データ拡張手法の比較\\n\\n節 2.2 で説明した BT, IBT, コンパラブル性を利用したIBT の 3 つの手法で実験を行い, データ拡張手法の比較を行った. データ拡張は学習データのサイズが少ないほど効果が見られるため, 学習に使用するデータ数を減らして実験を行った. ベースラインは対訳コーパスを 10 万文使用して学習を行った. 提案手法である 3 つのデータ拡張手法では, ベースラインに加え, 10 万文ずつコンパラブルコーパスからサンプリングし, データ拡張を行い, モデルを更新した. モデルの更新後, 新たに 10 万文をコンパラブルコーパスからサンプリングし, 対訳コーパスと混合してデータ拡張を行う. これを繰り返すことで, モデルの更新を進める. モデルの更新は 3 手法とも 5 回行った. 比較は, 開発データで最も高い BLEU スコアのモデルで比較を行った.\\n\\nデータ拡張手法の比較を行うために, BT, IBT, コンパラブル性を利用した IBT の 3 つの手法を行った. 実験の翻訳モデルの学習結果を, 表 3 に示す. なお, 表 3 の学習データサイズは, 左が対訳コーパスの使用文数, 右が単言語コーパスの使用文数となっている. なお, 太字になっている BLEU スコアが, 開発\\nデータで最も高い BLEUを示した Model である.英日方向における各手法の BLEU を比較すると， コンパラブル性を利用した IBT が最も性能が高く,続いて IBT の性能が高い. 日英方向における各手法の BLEU を比較すると, 英日と同じく,コンパラブル性を利用した IBT が最も性能が高く, 続いて IBT の性能が高い. IBT は, BT と比較して, BLEU が高いことが確認できる. コンパラブル性を利用した IBT は， コンパラブル性を利用していない BT や IBT と比較して, BLEUが高いことが確認できる.\\n\\n## 4 結論\\n\\n対訳コーパスをとして抽出されなかった文を含めたコンパラブルコーパスを利用してデータ拡張を行うことで, 翻訳モデルの性能が向上し, これまで利用されていなかったコンパラブルコーパスを活用することの有効性を確認した. また, コンパラブルコーパスの活用方法として, IBT を利用することの有効性と, 利用する単言語コーパスにコンパラブル性を持たせることの効果を確認することができた.\\n\\n## 謝辞\\n\\n本研究は JSPS 科研費 $18 \\\\mathrm{H} 01062$ の助成を受けた.\\n\\n## 参考文献\\n\\n[1] 内山将夫. 対訳データの効率的な構築方法. 情報通信研究機構季報 Vol.58, pp. 37-43, 2012.\\n\\n[2] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 86-96, 2016.\\n\\n[3] Vu Cong Duy Hoang, Phiilpp Koehn, Gholamreza Haffari, and Trevor Cohn. Iterative back-translation for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pp. 18-24, 2018.\\n\\n[4] Zhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and Enhong Chen. Joint training for neural machine translation models with monolingual data. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 555562, 2018.\\n\\n[5] 森田知熙, 秋葉友良, 塚田元. 双方向の逆翻訳を利用したニューラル機械翻訳の教師なし適応の検討. 情報処理学会研究報告 2018-NL-238 (第 5 回自然言語処理シンポジウム), pp. 1-5, 2018.\\n\\n[6] Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and Benjamin K. Tsou. Overview of the patent machine translation task at the NTCIR-10 workshop. Proceedings of the 10th NTCIR Conference, pp. 260-286, 2013.\\n\\n[7] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond`rej Bojar, Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pp. 177-180, 2007.\\n\\n[8] Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66-71, 2018.\\n\\n[9] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\\n\\n[10] Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp. 311-318, 2002.', 'category': 'NLP-2023', 'license': 'cc-by-4.0', 'credit': '(C) The Association for Natural Language Processing, (Licensed under CC BY 4.0）https://creativecommons.org/licenses/by/4.0/'}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"kunishou/J-ResearchCorpus\", split='train')\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"helloollel/vicuna-7b\"\n",
    "model_id = \"lmsys/vicuna-13b-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import bfloat16\n",
    "import transformers\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-bit quantization\n",
    "    bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "    bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [01:38<00:00, 101MB/s]\n",
      "pytorch_model-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [01:41<00:00, 97.6MB/s]\n",
      "pytorch_model-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [01:03<00:00, 97.2MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [04:24<00:00, 88.28s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.84s/it]\n",
      "generation_config.json: 100%|██████████| 192/192 [00:00<00:00, 744kB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model directly\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# vicuna 13b Tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# vicuna 13b Model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのテスト\n",
    "今回は要約タスクをさせます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    inputs = tokenizer.encode(\"あなたは教師です．以下の論文の内容を解釈し，生徒に説明してください．: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    print(f\"inputs:{inputs}\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=100, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([[    1, 29871, 30641, 30371, 30366, 30449, 31023, 31549, 30499, 30427,\n",
      "           242,   191,   145, 30651, 30557, 30199, 31871, 30333, 30199, 30728,\n",
      "         31294, 30396, 31201,   236,   138,   139, 30326, 30214, 30486,   232,\n",
      "           193,   149, 30353,   235,   173,   175, 30592, 30326, 30466, 30568,\n",
      "         30955, 30566, 30298,   242,   191,   145, 29901,   396, 29871, 30635,\n",
      "         30645, 30185, 30281, 30258, 31540,   233,   165,   179,   234,   194,\n",
      "           190,   235,   171,   182, 30353, 30697, 30807, 30332, 20504,  1230,\n",
      "          7437, 29899,  4300, 18411, 29871, 30396, 31107, 30406, 30326, 30366,\n",
      "         29871, 30459, 30203, 30715, 30281, 30582, 30258, 30459, 30185, 30715,\n",
      "         30255, 30199, 31704, 30406,    13,    13, 30329, 30346, 29871,   232,\n",
      "           135,   173, 31642, 29871, 31569, 31602, 29871, 31373, 31400, 29871,\n",
      "           232,   164,   157, 30395, 29871, 30824,    13,   235,   180,   141,\n",
      "         31392, 31615, 31438, 31030, 30415, 30257, 30415,    13, 10045, 29891,\n",
      "           314,   314,  3747, 29889, 29891, 19267, 29889,   558, 29892, 11208,\n",
      "         16912, 29889, 29873, 10730, 29891, 26434, 29889, 11178, 29892,   260,\n",
      "          2146, 29895,  1114, 29889, 29882,  1175,   603, 29889,  4415, 18105,\n",
      "         29992, 29873,   329, 29889, 16865,    13,    13,  2277, 29871,   233,\n",
      "           169,   133, 30698,    13,    13, 30635, 30645, 30185, 30281, 30258,\n",
      "         31540,   233,   165,   179,   234,   194,   190,   235,   171,   182,\n",
      "           313, 29940, 11490, 29897, 29871, 30199, 30415,   234,   194,   149,\n",
      "         30353, 30406, 30298, 30332,   232,   178,   193,   235,   171,   182,\n",
      "         30459, 30185, 30715, 30255, 30199,   233,   170,   142,   234,   178,\n",
      "           140, 30545, 30364, 30326, 30466, 29892, 29871, 30333, 30854,   232,\n",
      "           144,   155, 30956, 30499,   232,   178,   193,   232,   194,   159,\n",
      "         31689, 30807, 30513, 30553, 30366, 29871, 29906, 29871, 30773, 30199,\n",
      "         31243, 30968, 30199, 30459, 30185, 30715, 30255,   313, 30459, 30203,\n",
      "         30715, 30281, 30582, 30258, 30459, 30185, 30715, 30255, 29897, 29871,\n",
      "         30412, 30513, 30330,   232,   178,   193,   232,   194,   159, 31689,\n",
      "         30807, 30513, 30553, 30332, 30333, 31501, 30310, 30396, 30688, 31124,\n",
      "         30210, 30353,   233,   141,   192, 30544, 30427, 30332, 30880, 30545,\n",
      "         30458,   232,   189,   134, 30568,   233,   145,   164, 30406, 30566,\n",
      "         30553, 30466, 30298, 30332, 29889, 29871, 30326, 30412, 30326, 29892,\n",
      "         29871, 30333,   232,   144,   155, 30956, 30499, 31474,   232,   148,\n",
      "           182, 30458,   232,   178,   193,   232,   194,   159, 30427, 30332,\n",
      "         30723, 30199, 30449, 31022, 30371, 30568, 29892, 30923, 30568, 30199,\n",
      "         30333, 30449,   233,   141,   192, 30544, 30566, 30553, 31761,   233,\n",
      "           144,   171, 30466, 30513, 30553, 30466, 30326, 30441, 30465, 29889,\n",
      "         29871, 30346, 31367, 31455, 30499, 30449, 30330,   232,   178,   193,\n",
      "           235,   171,   182, 30459, 30185, 30715, 30255, 30364, 30326, 30466,\n",
      "           233,   141,   192, 30544, 30566, 30553, 30371, 30412, 30665, 30366,\n",
      "         30333, 30396,   232,   147,   174, 30954, 30466, 29892, 30459, 30203,\n",
      "         30715, 30281, 30582, 30258, 30459, 30185, 29871, 30715, 30255, 30753,\n",
      "         30988, 30396,   405, 11490, 29871, 30199, 30415,   234,   194,   149,\n",
      "         30353, 31704, 30406, 30427, 30332, 30880, 30545, 30396, 31302,   233,\n",
      "           164,   139, 30427, 30332, 29889, 29871,   235,   172,   152,   231,\n",
      "           193,   164, 31525,   236,   171,   150, 30353, 30787, 30453, 29892,\n",
      "         29871, 30459, 30203, 30715, 30281, 30582, 30258, 30459, 30185, 30715,\n",
      "         30255, 30499, 30597, 30185, 30369,   233,   142,   164, 31504, 30396,\n",
      "         30448, 30465, 30589, 30364, 31111, 29892, 29871, 30459, 30203, 30715,\n",
      "         30281, 30582, 30258, 30952, 30199, 31107, 30406, 29892, 20504,  1230,\n",
      "          7437, 29899,  4300, 18411, 29871, 30199, 31704, 30406, 30353, 30787,\n",
      "         30665, 30466]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1339: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample_text = dataset[0]['text']\n",
    "summary = summarize_text(sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたは教師です．以下の論文の内容を解釈し，生徒に説明してください．: # ニューラル機械翻訳における Iterative Back-Translation を利用した コンパラブルコーパスの活用\n",
      "\n",
      "山本 優紀 秋葉 友良 塚田 元\n",
      "豊橋技術科学大学\n",
      "\\{yamamoto.yuki.pr, akiba.tomoyoshi.tk, tsukada.hajime.hl\\}@tut.jp\n",
      "\n",
      "## 概要\n",
      "\n",
      "ニューラル機械翻訳 (NMT) の学習に用いる対訳コーパスの構築法として, 文書単位で対応付けられた 2 つの言語のコーパス (コンパラブルコーパス) から、対応付けられる文ペアを自動的に抽出する手法が広く採用されている. しかし, 文単位で意味が対応するものは少なく,多くの文は抽出されず捨てられてしまう. 本研究では、対訳コーパスとして抽出されなかった文を含めて,コンパラブルコー パス全体を NMT の学習に活用する手法を提案する. 評価実験により, コンパラブルコーパスでデータ拡張を行うことや, コンパラブル性の利用, Iterative Back-Translation の活用によって, NMT の性能が向上することが示された.\n",
      "\n",
      "## 解説\n",
      "\n",
      "この論文は、ニューラル機械翻訳の学習に用いる対訳コーパスの構築法について説明しています。対訳コー\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
